{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b765f9f6",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13ca1e",
   "metadata": {},
   "source": [
    "#### 1) Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee42e2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\amuly\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\amuly\\anaconda3\\lib\\site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\amuly\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\amuly\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\amuly\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amuly\\anaconda3\\lib\\site-packages (from requests) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amuly\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\amuly\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8e36238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20902fb4",
   "metadata": {},
   "source": [
    "Sending request to the web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab0f9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_tags(url):\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    \n",
    "    headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    \n",
    "    header_list = []\n",
    "    for header in headers:\n",
    "        header_list.append(header.text.strip())\n",
    "\n",
    "    # creating a pandas DataFrame from the header list\n",
    "    df = pd.DataFrame(header_list, columns=['Header'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a55c0136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "df = header_tags('https://en.wikipedia.org/wiki/Main_Page')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b64099",
   "metadata": {},
   "source": [
    "#### 2) Write a python program to display IMDB’s Top rated 50 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3676a766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.imdb.com/chart/top/'\n",
    "r = requests.get(url)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92f71d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_50_movies(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    movie_containers = soup.find_all('td', class_='titleColumn')\n",
    "\n",
    "    names = []\n",
    "    ratings = []\n",
    "    years = []\n",
    "\n",
    "    for container in movie_containers[:50]:\n",
    "        # movie name\n",
    "        name = container.a.text\n",
    "        names.append(name)\n",
    "    \n",
    "        # movie rating\n",
    "        rating = float(container.parent.find('td', class_='ratingColumn imdbRating').text.strip())\n",
    "        ratings.append(rating)\n",
    "\n",
    "        # year of release\n",
    "        year = int(container.span.text.strip('()'))\n",
    "        years.append(year)\n",
    "\n",
    "    # DataFrame \n",
    "    data = {'Name': names, 'Rating': ratings, 'Year': years}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b984714c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>8.9</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Il buono, il brutto, il cattivo</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Lord of the Rings: The Two Towers</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Inception</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Empire Strikes Back</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Matrix</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GoodFellas</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>One Flew Over the Cuckoo's Nest</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Se7en</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's a Wonderful Life</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Shichinin no samurai</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Silence of the Lambs</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Saving Private Ryan</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Cidade de Deus</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Interstellar</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>La vita è bella</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The Green Mile</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Star Wars</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Terminator 2: Judgment Day</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Back to the Future</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Sen to Chihiro no kamikakushi</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>The Pianist</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Psycho</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Gisaengchung</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Léon</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>The Lion King</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Gladiator</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>American History X</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The Departed</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Whiplash</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The Prestige</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>The Usual Suspects</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Casablanca</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Hotaru no haka</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Seppuku</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>The Intouchables</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Modern Times</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Once Upon a Time in the West</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Nuovo Cinema Paradiso</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Rear Window</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  Rating  Year\n",
       "0                            The Shawshank Redemption     9.2  1994\n",
       "1                                       The Godfather     9.2  1972\n",
       "2                                     The Dark Knight     9.0  2008\n",
       "3                               The Godfather Part II     9.0  1974\n",
       "4                                        12 Angry Men     9.0  1957\n",
       "5                                    Schindler's List     8.9  1993\n",
       "6       The Lord of the Rings: The Return of the King     8.9  2003\n",
       "7                                        Pulp Fiction     8.8  1994\n",
       "8   The Lord of the Rings: The Fellowship of the Ring     8.8  2001\n",
       "9                     Il buono, il brutto, il cattivo     8.8  1966\n",
       "10                                       Forrest Gump     8.8  1994\n",
       "11                                         Fight Club     8.7  1999\n",
       "12              The Lord of the Rings: The Two Towers     8.7  2002\n",
       "13                                          Inception     8.7  2010\n",
       "14                            The Empire Strikes Back     8.7  1980\n",
       "15                                         The Matrix     8.7  1999\n",
       "16                                         GoodFellas     8.7  1990\n",
       "17                    One Flew Over the Cuckoo's Nest     8.6  1975\n",
       "18                                              Se7en     8.6  1995\n",
       "19                              It's a Wonderful Life     8.6  1946\n",
       "20                               Shichinin no samurai     8.6  1954\n",
       "21                           The Silence of the Lambs     8.6  1991\n",
       "22                                Saving Private Ryan     8.6  1998\n",
       "23                                     Cidade de Deus     8.6  2002\n",
       "24                                       Interstellar     8.6  2014\n",
       "25                                    La vita è bella     8.6  1997\n",
       "26                                     The Green Mile     8.6  1999\n",
       "27                                          Star Wars     8.5  1977\n",
       "28                         Terminator 2: Judgment Day     8.5  1991\n",
       "29                                 Back to the Future     8.5  1985\n",
       "30                      Sen to Chihiro no kamikakushi     8.5  2001\n",
       "31                                        The Pianist     8.5  2002\n",
       "32                                             Psycho     8.5  1960\n",
       "33                                       Gisaengchung     8.5  2019\n",
       "34                                               Léon     8.5  1994\n",
       "35                                      The Lion King     8.5  1994\n",
       "36                                          Gladiator     8.5  2000\n",
       "37                                 American History X     8.5  1998\n",
       "38                                       The Departed     8.5  2006\n",
       "39                                           Whiplash     8.5  2014\n",
       "40                                       The Prestige     8.5  2006\n",
       "41                                 The Usual Suspects     8.5  1995\n",
       "42                                         Casablanca     8.5  1942\n",
       "43                                     Hotaru no haka     8.5  1988\n",
       "44                                            Seppuku     8.5  1962\n",
       "45                                   The Intouchables     8.5  2011\n",
       "46                                       Modern Times     8.4  1936\n",
       "47                       Once Upon a Time in the West     8.4  1968\n",
       "48                              Nuovo Cinema Paradiso     8.4  1988\n",
       "49                                        Rear Window     8.4  1954"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = imdb_50_movies('https://www.imdb.com/chart/top/?ref_=nv_mv_50')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35371c63",
   "metadata": {},
   "source": [
    "#### 3) Write a python program to display IMDB’s Top rated 50 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0b0946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_50_movies_india(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    movie_containers = soup.find_all('td', class_='titleColumn')\n",
    "\n",
    "    names = []\n",
    "    ratings = []\n",
    "    years = []\n",
    "\n",
    "    for container in movie_containers[:50]:\n",
    "        # movie name\n",
    "        name = container.a.text\n",
    "        names.append(name)\n",
    "    \n",
    "        # movie rating\n",
    "        rating = float(container.parent.find('td', class_='ratingColumn imdbRating').text.strip())\n",
    "        ratings.append(rating)\n",
    "\n",
    "        # year of release\n",
    "        year = int(container.span.text.strip('()'))\n",
    "        years.append(year)\n",
    "\n",
    "    # DataFrame \n",
    "    data = {'Name': names, 'Rating': ratings, 'Year': years}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2baf8f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gol Maal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>777 Charlie</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pariyerum Perumal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apur Sansar</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Manichitrathazhu</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jai Bhim</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#Home</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Soorarai Pottru</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Black Friday</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kumbalangi Nights</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C/o Kancharapalem</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Taare Zameen Par</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kireedam</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Dangal</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Kaithi</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Jersey</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>96</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Maya Bazaar</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Natsamrat</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Asuran</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Drishyam 2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sita Ramam</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Thevar Magan</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Visaaranai</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sarpatta Parambarai</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Thalapathi</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Nadodikkattu</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Pather Panchali</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Drishyam</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Thani Oruvan</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Jaane Bhi Do Yaaro</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Vada Chennai</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Aparajito</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Khosla Ka Ghosla!</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Sardar Udham</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Anniyan</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Ratsasan</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Chupke Chupke</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Gangs of Wasseypur</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Drishyam</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Mahanati</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Peranbu</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Bangalore Days</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Premam</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Satya</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Name  Rating  Year\n",
       "0   Ramayana: The Legend of Prince Rama     8.6  1993\n",
       "1            Rocketry: The Nambi Effect     8.4  2022\n",
       "2                               Nayakan     8.4  1987\n",
       "3                              Gol Maal     8.4  1979\n",
       "4                           777 Charlie     8.4  2022\n",
       "5                     Pariyerum Perumal     8.4  2018\n",
       "6                            Anbe Sivam     8.4  2003\n",
       "7                           Apur Sansar     8.4  1959\n",
       "8                              3 Idiots     8.4  2009\n",
       "9                      Manichitrathazhu     8.3  1993\n",
       "10                             Jai Bhim     8.3  2021\n",
       "11                                #Home     8.3  2021\n",
       "12                      Soorarai Pottru     8.3  2020\n",
       "13                         Black Friday     8.3  2004\n",
       "14                    Kumbalangi Nights     8.3  2019\n",
       "15                    C/o Kancharapalem     8.3  2018\n",
       "16                     Taare Zameen Par     8.3  2007\n",
       "17                             Kireedam     8.3  1989\n",
       "18                               Dangal     8.3  2016\n",
       "19                               Kaithi     8.3  2019\n",
       "20                               Jersey     8.3  2019\n",
       "21                                   96     8.3  2018\n",
       "22                          Maya Bazaar     8.2  1957\n",
       "23                            Natsamrat     8.2  2016\n",
       "24                               Asuran     8.2  2019\n",
       "25                           Drishyam 2     8.2  2021\n",
       "26                           Sita Ramam     8.2  2022\n",
       "27                         Thevar Magan     8.2  1992\n",
       "28                           Visaaranai     8.2  2015\n",
       "29                  Sarpatta Parambarai     8.2  2021\n",
       "30                           Thalapathi     8.2  1991\n",
       "31                         Nadodikkattu     8.2  1987\n",
       "32                      Pather Panchali     8.2  1955\n",
       "33                             Drishyam     8.2  2013\n",
       "34                         Thani Oruvan     8.2  2015\n",
       "35                   Jaane Bhi Do Yaaro     8.2  1983\n",
       "36                         Vada Chennai     8.2  2018\n",
       "37                            Aparajito     8.2  1956\n",
       "38                    Khosla Ka Ghosla!     8.2  2006\n",
       "39                         Sardar Udham     8.2  2021\n",
       "40                              Anniyan     8.2  2005\n",
       "41                             Ratsasan     8.1  2018\n",
       "42                        Chupke Chupke     8.1  1975\n",
       "43                   Gangs of Wasseypur     8.1  2012\n",
       "44                             Drishyam     8.1  2015\n",
       "45                             Mahanati     8.1  2018\n",
       "46                              Peranbu     8.1  2018\n",
       "47                       Bangalore Days     8.1  2014\n",
       "48                               Premam     8.1  2015\n",
       "49                                Satya     8.1  1998"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = imdb_50_movies('https://www.imdb.com/india/top-rated-indian-movies/?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=69568c7e-01d9-48cb-b6bc-c7c88087a621&pf_rd_r=FB3MNJ9GPTS7AQX36GMH&pf_rd_s=center-1&pf_rd_t=60601&pf_rd_i=india.toprated&ref_=fea_india_ss_toprated_india_tr_india250_hd')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85e698",
   "metadata": {},
   "source": [
    "#### 4) Write a python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) from https://presidentofindia.nic.in/former-presidents.htm and make data frame.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "924390e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def former_presidents(url):    \n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    president_name = soup.find_all('div', class_='presidentListing')\n",
    "    term_of_office = soup.find_all('span', class_='terms')\n",
    "\n",
    "\n",
    "    president_name = soup.find_all('div', class_='presidentListing')\n",
    "    term_of_office = soup.find_all('span', class_='terms')\n",
    "\n",
    "    names = []\n",
    "    terms = []\n",
    "\n",
    "    for i in president_name:\n",
    "        name = i.text.split('(')[0].strip()\n",
    "        if name != '':\n",
    "            names.append(name)\n",
    "\n",
    "    for e in soup.select('li:has(h3)'):\n",
    "        terms.append(e.span.next_sibling )\n",
    "\n",
    "\n",
    "    data = {'Name': names, 'Term of Office': terms}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "990aa365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Term of Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Name  \\\n",
       "0           Shri Ram Nath Kovind   \n",
       "1          Shri Pranab Mukherjee   \n",
       "2   Smt Pratibha Devisingh Patil   \n",
       "3         DR. A.P.J. Abdul Kalam   \n",
       "4           Shri K. R. Narayanan   \n",
       "5        Dr Shankar Dayal Sharma   \n",
       "6            Shri R Venkataraman   \n",
       "7               Giani Zail Singh   \n",
       "8      Shri Neelam Sanjiva Reddy   \n",
       "9       Dr. Fakhruddin Ali Ahmed   \n",
       "10  Shri Varahagiri Venkata Giri   \n",
       "11              Dr. Zakir Husain   \n",
       "12  Dr. Sarvepalli Radhakrishnan   \n",
       "13           Dr. Rajendra Prasad   \n",
       "\n",
       "                                       Term of Office  \n",
       "0                     25 July, 2017 to 25 July, 2022   \n",
       "1                     25 July, 2012 to 25 July, 2017   \n",
       "2                     25 July, 2007 to 25 July, 2012   \n",
       "3                     25 July, 2002 to 25 July, 2007   \n",
       "4                     25 July, 1997 to 25 July, 2002   \n",
       "5                     25 July, 1992 to 25 July, 1997   \n",
       "6                     25 July, 1987 to 25 July, 1992   \n",
       "7                     25 July, 1982 to 25 July, 1987   \n",
       "8                     25 July, 1977 to 25 July, 1982   \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10   3 May, 1969 to 20 July, 1969 and 24 August, 1...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = former_presidents('https://presidentofindia.nic.in/former-presidents.htm')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40426cd",
   "metadata": {},
   "source": [
    "#### 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf553c",
   "metadata": {},
   "source": [
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afb3880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_odi_teams(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    team_table = soup.find('table', class_='table')\n",
    "    team_data = []\n",
    "    rows = team_table.find_all('tr')\n",
    "\n",
    "    for row in rows[1:11]:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip() for col in cols]\n",
    "        team_data.append(cols)\n",
    "\n",
    "    team_df = pd.DataFrame(team_data, columns=['Position', 'Team', 'Matches', 'Points', 'Rating'])\n",
    "    return team_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1f956bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Team</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "      <td>35</td>\n",
       "      <td>3,965</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>31</td>\n",
       "      <td>3,504</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>India</td>\n",
       "      <td>47</td>\n",
       "      <td>5,294</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>England</td>\n",
       "      <td>36</td>\n",
       "      <td>3,988</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>25</td>\n",
       "      <td>2,649</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>31</td>\n",
       "      <td>3,141</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>38</td>\n",
       "      <td>3,625</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>36</td>\n",
       "      <td>3,099</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>West Indies</td>\n",
       "      <td>43</td>\n",
       "      <td>3,105</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>20</td>\n",
       "      <td>1,419</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Position          Team Matches Points Rating\n",
       "0        1     Australia      35  3,965    113\n",
       "1        2   New Zealand      31  3,504    113\n",
       "2        3         India      47  5,294    113\n",
       "3        4       England      36  3,988    111\n",
       "4        5      Pakistan      25  2,649    106\n",
       "5        6  South Africa      31  3,141    101\n",
       "6        7    Bangladesh      38  3,625     95\n",
       "7        8     Sri Lanka      36  3,099     86\n",
       "8        9   West Indies      43  3,105     72\n",
       "9       10   Afghanistan      20  1,419     71"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "team_df = get_top_10_odi_teams(url)\n",
    "team_df['Team'] = team_df['Team'].str.split('\\n').str[0]\n",
    "team_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d90fce",
   "metadata": {},
   "source": [
    "b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "23f786f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_10_batsmen_odi(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    batsman = soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "    batsman2 = soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "    batsman_name = []\n",
    "\n",
    "    team_name = soup.find_all('span', class_='rankings-block__banner--nation')\n",
    "    country = []\n",
    "\n",
    "    rating_ = soup.find_all('div', class_='rankings-block__banner--rating')\n",
    "    rating2 = soup.find_all('td',class_='table-body__cell rating')\n",
    "    ratings = []\n",
    "\n",
    "    for i in batsman:\n",
    "        battingmen = i.text.strip()\n",
    "        batsman_name.append(battingmen)    \n",
    "\n",
    "    batsman_name\n",
    "\n",
    "    for i in batsman2: \n",
    "        bat_men = i.text.strip()\n",
    "        batsman_name.append(bat_men)\n",
    "\n",
    "\n",
    "    for e in rating_:\n",
    "        rate = e.text.strip()\n",
    "        ratings.append(rate)\n",
    "\n",
    "    for e in rating2:\n",
    "        rates = e.text.strip()\n",
    "        ratings.append(rates)\n",
    "\n",
    "\n",
    "\n",
    "    data = {'Name': batsman_name, 'Rating': ratings}\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ec52bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Name Rating\n",
      "0             Babar Azam    887\n",
      "1  Rassie van der Dussen    777\n",
      "2            Imam-ul-Haq    740\n",
      "3           Shubman Gill    738\n",
      "4           David Warner    726\n",
      "5            Virat Kohli    719\n",
      "6        Quinton de Kock    718\n",
      "7           Rohit Sharma    707\n",
      "8            Steve Smith    702\n",
      "9           Fakhar Zaman    699\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "odi_batsmen = top_10_batsmen_odi(url)\n",
    "odi_batsmen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cabfd5",
   "metadata": {},
   "source": [
    "c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d065e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_10_bowlers_odi(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    bowler = soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "    bowler2 = soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "    bowler_name = []\n",
    "\n",
    "    team_name = soup.find_all('span', class_='rankings-block__banner--nation')\n",
    "    country = []\n",
    "\n",
    "    rating_ = soup.find_all('div', class_='rankings-block__banner--rating')\n",
    "    rating2 = soup.find_all('td',class_='table-body__cell rating')\n",
    "    ratings = []\n",
    "\n",
    "    for i in bowler:\n",
    "        bowl = i.text.strip()\n",
    "        bowler_name.append(bowl)    \n",
    "\n",
    "    bowler_name\n",
    "\n",
    "    for i in bowler2: \n",
    "        bowl_men = i.text.strip()\n",
    "        bowler_name.append(bowl_men)\n",
    "\n",
    "\n",
    "    for e in rating_:\n",
    "        rate = e.text.strip()\n",
    "        ratings.append(rate)\n",
    "\n",
    "    for e in rating2:\n",
    "        rates = e.text.strip()\n",
    "        ratings.append(rates)\n",
    "\n",
    "\n",
    "\n",
    "    data = {'Name': bowler_name, 'Rating': ratings}\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "494e4a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Name Rating\n",
      "0    Josh Hazlewood    705\n",
      "1       Trent Boult    694\n",
      "2    Mohammed Siraj    691\n",
      "3    Mitchell Starc    686\n",
      "4        Matt Henry    676\n",
      "5       Rashid Khan    659\n",
      "6        Adam Zampa    652\n",
      "7    Shaheen Afridi    641\n",
      "8  Mujeeb Ur Rahman    637\n",
      "9   Shakib Al Hasan    636\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "odi_bowlers = top_10_bowlers_odi(url)\n",
    "odi_bowlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a8ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6db43e5",
   "metadata": {},
   "source": [
    "#### Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9a360619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_odi_teams(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    team_table = soup.find('table', class_='table')\n",
    "    team_data = []\n",
    "    rows = team_table.find_all('tr')\n",
    "\n",
    "    for row in rows[1:11]:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip() for col in cols]\n",
    "        team_data.append(cols)\n",
    "\n",
    "    team_df = pd.DataFrame(team_data, columns=['Position', 'Team', 'Matches', 'Points', 'Rating'])\n",
    "    return team_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8472f28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Team</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "      <td>21</td>\n",
       "      <td>3,603</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>England</td>\n",
       "      <td>28</td>\n",
       "      <td>3,342</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>26</td>\n",
       "      <td>3,098</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>India</td>\n",
       "      <td>27</td>\n",
       "      <td>2,820</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>25</td>\n",
       "      <td>2,553</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Indies</td>\n",
       "      <td>27</td>\n",
       "      <td>2,535</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>13</td>\n",
       "      <td>983</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>11</td>\n",
       "      <td>821</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>27</td>\n",
       "      <td>1,678</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>8</td>\n",
       "      <td>353</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Position          Team Matches Points Rating\n",
       "0        1     Australia      21  3,603    172\n",
       "1        2       England      28  3,342    119\n",
       "2        3  South Africa      26  3,098    119\n",
       "3        4         India      27  2,820    104\n",
       "4        5   New Zealand      25  2,553    102\n",
       "5        6   West Indies      27  2,535     94\n",
       "6        7    Bangladesh      13    983     76\n",
       "7        8      Thailand      11    821     75\n",
       "8        9      Pakistan      27  1,678     62\n",
       "9       10     Sri Lanka       8    353     44"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "team_df = get_top_10_odi_teams(url)\n",
    "team_df['Team'] = team_df['Team'].str.split('\\n').str[0]\n",
    "team_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309a1de",
   "metadata": {},
   "source": [
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "dede238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_batswomen_odi(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    batsman = soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "    batsman2 = soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "    batsman_name = []\n",
    "\n",
    "    team_name = soup.find_all('span', class_='rankings-block__banner--nation')\n",
    "    country = []\n",
    "\n",
    "    rating_ = soup.find_all('div', class_='rankings-block__banner--rating')\n",
    "    rating2 = soup.find_all('td',class_='table-body__cell rating')\n",
    "    ratings = []\n",
    "\n",
    "    for i in batsman:\n",
    "        battingmen = i.text.strip()\n",
    "        batsman_name.append(battingmen)    \n",
    "\n",
    "    batsman_name\n",
    "\n",
    "    for i in batsman2: \n",
    "        bat_men = i.text.strip()\n",
    "        batsman_name.append(bat_men)\n",
    "\n",
    "\n",
    "    for e in rating_:\n",
    "        rate = e.text.strip()\n",
    "        ratings.append(rate)\n",
    "\n",
    "    for e in rating2:\n",
    "        rates = e.text.strip()\n",
    "        ratings.append(rates)\n",
    "\n",
    "\n",
    "\n",
    "    data = {'Name': batsman_name, 'Rating': ratings}\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "db44924a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name Rating\n",
      "0         Alyssa Healy    762\n",
      "1          Beth Mooney    754\n",
      "2      Laura Wolvaardt    732\n",
      "3       Natalie Sciver    731\n",
      "4          Meg Lanning    717\n",
      "5     Harmanpreet Kaur    716\n",
      "6      Smriti Mandhana    714\n",
      "7  Chamari Athapaththu    655\n",
      "8    Amy Satterthwaite    641\n",
      "9         Ellyse Perry    626\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "odi_batswomen = top_10_batswomen_odi(url)\n",
    "odi_batswomen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24001b93",
   "metadata": {},
   "source": [
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "79de4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_women_bowlers_odi(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    bowler = soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "    bowler2 = soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "    bowler_name = []\n",
    "\n",
    "    team_name = soup.find_all('span', class_='rankings-block__banner--nation')\n",
    "    country = []\n",
    "\n",
    "    rating_ = soup.find_all('div', class_='rankings-block__banner--rating')\n",
    "    rating2 = soup.find_all('td',class_='table-body__cell rating')\n",
    "    ratings = []\n",
    "\n",
    "    for i in bowler:\n",
    "        bowl = i.text.strip()\n",
    "        bowler_name.append(bowl)    \n",
    "\n",
    "    bowler_name\n",
    "\n",
    "    for i in bowler2: \n",
    "        bowl_men = i.text.strip()\n",
    "        bowler_name.append(bowl_men)\n",
    "\n",
    "\n",
    "    for e in rating_:\n",
    "        rate = e.text.strip()\n",
    "        ratings.append(rate)\n",
    "\n",
    "    for e in rating2:\n",
    "        rates = e.text.strip()\n",
    "        ratings.append(rates)\n",
    "\n",
    "\n",
    "\n",
    "    data = {'Name': bowler_name, 'Rating': ratings}\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "614bb6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name Rating\n",
      "0    Sophie Ecclestone    751\n",
      "1        Jess Jonassen    723\n",
      "2       Shabnim Ismail    722\n",
      "3         Megan Schutt    704\n",
      "4      Hayley Matthews    660\n",
      "5           Kate Cross    655\n",
      "6       Ayabonga Khaka    634\n",
      "7  Rajeshwari Gayakwad    617\n",
      "8       Marizanne Kapp    598\n",
      "9        Deepti Sharma    589\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling'\n",
    "odi_bowlers = top_10_women_bowlers_odi(url)\n",
    "odi_bowlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224caa10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c04e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebbaed5b",
   "metadata": {},
   "source": [
    "#### 7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame i) Headline ii) Time iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f68be288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline Time  \\\n",
      "0   What Home Depot's billion-dollar pay raise may...  N/A   \n",
      "1   Sales, spin-offs and splits — the differences ...  N/A   \n",
      "2   Charlie Munger reportedly warns of trouble for...  N/A   \n",
      "3   Workers are secretly using ChatGPT, AI and it ...  N/A   \n",
      "4   A psychologist shares 6 toxic phrases 'highly ...  N/A   \n",
      "5   Top cloud providers Amazon, Microsoft and Goog...  N/A   \n",
      "6   Alzheimer’s patients may wait years for new tr...  N/A   \n",
      "7       Retirees are flocking to these 10 U.S. states  N/A   \n",
      "8   It might be Jonah Peretti's last chance to tur...  N/A   \n",
      "9   Obesity is a chronic disease. It's why treatin...  N/A   \n",
      "10  Mario Gabelli ranks old media stocks among his...  N/A   \n",
      "11  Earnings playbook: The second half of the seas...  N/A   \n",
      "12  SpaceX to spend $2 billion on Starship this ye...  N/A   \n",
      "13  Jack Dorsey criticizes Elon Musk’s leadership ...  N/A   \n",
      "14  Banks including JPMorgan Chase, Bank of Americ...  N/A   \n",
      "15  How to raise a kids with a ‘secure’ attachment...  N/A   \n",
      "16  The business case for green sports stadiums an...  N/A   \n",
      "17  A solid start to earnings season will be put t...  N/A   \n",
      "18  TikTokers are using beef tallow to treat acne:...  N/A   \n",
      "19  This 28-year-old pays $62 a month to live in a...  N/A   \n",
      "20  As part of the  ‘cocktail culture,' consumers ...  N/A   \n",
      "21  Why GM is killing the Chevy Bolt — America's c...  N/A   \n",
      "22  Just a few stocks are behind the market's resi...  N/A   \n",
      "23  Google Cloud's Kurian on road to profit: 'We w...  N/A   \n",
      "24  Olipop nears $200 million in annual sales—and ...  N/A   \n",
      "25  This 25-year-old makes $200/hour without a bac...  N/A   \n",
      "26  The most overbought and oversold S&P 500 stock...  N/A   \n",
      "27  Mark Cuban says paying Twitter for a blue chec...  N/A   \n",
      "28  Goldman Sachs says these are its top picks com...  N/A   \n",
      "29  What’s next for SpaceX’s Starship after a dram...  N/A   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2023/04/30/what-home-depo...  \n",
      "1   https://www.cnbc.com/2023/04/30/sales-spin-off...  \n",
      "2   https://www.cnbc.com/2023/04/30/charlie-munger...  \n",
      "3   https://www.cnbc.com/2023/04/30/the-big-cyber-...  \n",
      "4   https://www.cnbc.com/2023/04/30/psychologist-s...  \n",
      "5   https://www.cnbc.com/2023/04/30/cloud-provider...  \n",
      "6   https://www.cnbc.com/2023/04/30/alzheimers-tre...  \n",
      "7   https://www.cnbc.com/2023/04/30/florida-is-the...  \n",
      "8   https://www.cnbc.com/2023/04/30/buzzfeed-turna...  \n",
      "9   https://www.cnbc.com/2023/04/30/obesity-is-a-c...  \n",
      "10  https://www.cnbc.com/2023/04/30/mario-gabelli-...  \n",
      "11  https://www.cnbc.com/2023/04/30/earnings-playb...  \n",
      "12  https://www.cnbc.com/2023/04/29/elon-musk-spac...  \n",
      "13  https://www.cnbc.com/2023/04/29/jack-dorsey-cr...  \n",
      "14  https://www.cnbc.com/2023/04/29/first-republic...  \n",
      "15  https://www.cnbc.com/2023/04/29/how-to-raise-a...  \n",
      "16  https://www.cnbc.com/2023/04/29/the-business-c...  \n",
      "17  https://www.cnbc.com/2023/04/29/decent-start-t...  \n",
      "18  https://www.cnbc.com/2023/04/29/tiktokers-are-...  \n",
      "19  https://www.cnbc.com/2023/04/29/28-year-old-pa...  \n",
      "20  https://www.cnbc.com/2023/04/29/cocktail-cultu...  \n",
      "21  https://www.cnbc.com/2023/04/29/why-gm-is-kill...  \n",
      "22  https://www.cnbc.com/2023/04/29/just-a-few-sto...  \n",
      "23  https://www.cnbc.com/2023/04/29/google-cloud-b...  \n",
      "24  https://www.cnbc.com/2023/04/29/olipop-nears-2...  \n",
      "25  https://www.cnbc.com/2023/04/29/25-year-old-no...  \n",
      "26  https://www.cnbc.com/2023/04/29/herea-re-the-m...  \n",
      "27  https://www.cnbc.com/2023/04/29/mark-cuban-pay...  \n",
      "28  https://www.cnbc.com/2023/04/29/goldman-sachs-...  \n",
      "29  https://www.cnbc.com/2023/04/29/spacex-starshi...  \n"
     ]
    }
   ],
   "source": [
    "def scrape_cnbc_news(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    headlines = []\n",
    "    times = []\n",
    "    news_links = []\n",
    "\n",
    "    for article in soup.find_all('a', class_='LatestNews-headline'):\n",
    "        headlines.append(article.text.strip())\n",
    "\n",
    "    for time in soup.find_all('time', class_='LatestNews-timestamp'):\n",
    "        if 'datetime' in time.attrs:\n",
    "            times.append(time['datetime'])\n",
    "        else:\n",
    "            times.append('N/A')\n",
    "\n",
    "    for link in soup.find_all('a', class_='LatestNews-headline'):\n",
    "        news_links.append(link['href'])\n",
    "\n",
    "    df = pd.DataFrame({'Headline': headlines, 'Time': times, 'News Link': news_links})\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = \"https://www.cnbc.com/world/?region=world\"\n",
    "    df = scrape_cnbc_news(url)\n",
    "    print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85635e",
   "metadata": {},
   "source": [
    "8) Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "98a757ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>[October, 2021]</td>\n",
       "      <td>#skip-to-content-anchor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>[October, 2021]</td>\n",
       "      <td>http://www.elsevier.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>[October, 2015]</td>\n",
       "      <td>https://account.elsevier.com/auth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>[August, 1998]</td>\n",
       "      <td>https://elsevier.com/about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>[June, 2017]</td>\n",
       "      <td>https://www.elsevier.com/connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>[February, 2019]</td>\n",
       "      <td>https://www.elsevier.com/about/careers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>[April, 2021]</td>\n",
       "      <td>https://elsevier.com/about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>[February, 2015]</td>\n",
       "      <td>https://www.elsevier.com/connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>[August, 1999]</td>\n",
       "      <td>https://www.elsevier.com/about/careers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>[March, 2020]</td>\n",
       "      <td>https://www.elsevier.com/rd-solutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>[February, 2021]</td>\n",
       "      <td>https://www.elsevier.com/clinical-solutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>[October, 2007]</td>\n",
       "      <td>https://www.elsevier.com/research-platforms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>[August, 2016]</td>\n",
       "      <td>https://www.elsevier.com/research-intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>[April, 2021]</td>\n",
       "      <td>https://www.elsevier.com/education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>[December, 1997]</td>\n",
       "      <td>https://www.elsevier.com/solutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>[August, 2021]</td>\n",
       "      <td>https://www.elsevier.com/rd-solutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>[September, 2021]</td>\n",
       "      <td>https://www.elsevier.com/clinical-solutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>[June, 2021]</td>\n",
       "      <td>https://www.elsevier.com/research-platforms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>[December, 2016]</td>\n",
       "      <td>https://www.elsevier.com/research-intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>[September, 2021]</td>\n",
       "      <td>https://www.elsevier.com/education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>[May, 2021]</td>\n",
       "      <td>https://www.elsevier.com/solutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>[January, 2014]</td>\n",
       "      <td>https://www.elsevier.com/authors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>[December, 1997]</td>\n",
       "      <td>https://www.elsevier.com/editors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>[October, 2021]</td>\n",
       "      <td>https://www.elsevier.com/reviewers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>[February, 2010]</td>\n",
       "      <td>https://www.elsevier.com/librarians</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors     Published Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    [October, 2021]   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    [October, 2021]   \n",
       "2                    Prakken, Henry, Sartor, Giovanni    [October, 2015]   \n",
       "3                                  Boden, Margaret A.     [August, 1998]   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       [June, 2017]   \n",
       "5                                         Miller, Tim   [February, 2019]   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      [April, 2021]   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   [February, 2015]   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     [August, 1999]   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      [March, 2020]   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   [February, 2021]   \n",
       "11                Bench-Capon, T.J.M., Dunne, Paul E.    [October, 2007]   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     [August, 2016]   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      [April, 2021]   \n",
       "14                       Blum, Avrim L., Langley, Pat   [December, 1997]   \n",
       "15                    Arora, Saurabh, Doshi, Prashant     [August, 2021]   \n",
       "16       Aas, Kjersti, Jullum, Martin, Løland, Anders  [September, 2021]   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       [June, 2021]   \n",
       "18     Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.   [December, 2016]   \n",
       "19                       Riveiro, Maria, Thill, Serge  [September, 2021]   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        [May, 2021]   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    [January, 2014]   \n",
       "22                       Kohavi, Ron, John, George H.   [December, 1997]   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    [October, 2021]   \n",
       "24                                    Ying, Mingsheng   [February, 2010]   \n",
       "\n",
       "                                         Paper URL  \n",
       "0                          #skip-to-content-anchor  \n",
       "1                          http://www.elsevier.com  \n",
       "2                https://account.elsevier.com/auth  \n",
       "3                       https://elsevier.com/about  \n",
       "4                 https://www.elsevier.com/connect  \n",
       "5           https://www.elsevier.com/about/careers  \n",
       "6                       https://elsevier.com/about  \n",
       "7                 https://www.elsevier.com/connect  \n",
       "8           https://www.elsevier.com/about/careers  \n",
       "9            https://www.elsevier.com/rd-solutions  \n",
       "10     https://www.elsevier.com/clinical-solutions  \n",
       "11     https://www.elsevier.com/research-platforms  \n",
       "12  https://www.elsevier.com/research-intelligence  \n",
       "13              https://www.elsevier.com/education  \n",
       "14              https://www.elsevier.com/solutions  \n",
       "15           https://www.elsevier.com/rd-solutions  \n",
       "16     https://www.elsevier.com/clinical-solutions  \n",
       "17     https://www.elsevier.com/research-platforms  \n",
       "18  https://www.elsevier.com/research-intelligence  \n",
       "19              https://www.elsevier.com/education  \n",
       "20              https://www.elsevier.com/solutions  \n",
       "21                https://www.elsevier.com/authors  \n",
       "22                https://www.elsevier.com/editors  \n",
       "23              https://www.elsevier.com/reviewers  \n",
       "24             https://www.elsevier.com/librarians  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    \n",
    "Paper_Title = []\n",
    "Authors = []\n",
    "Published_Date = []\n",
    "Paper_URL = []\n",
    "    \n",
    "title = soup.find_all('h2',class_='sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg')\n",
    "author = soup.find_all('span', class_='sc-1w3fpd7-0 dnCnAO')\n",
    "published_date = soup.find_all('span', class_='sc-1thf9ly-2 dvggWt')\n",
    "paper_url = soup.find_all('a',class_='sc-5smygv-0 fIXTHm')\n",
    "\n",
    "for i in title:\n",
    "    Paper_Title.append(i.text.strip())\n",
    "\n",
    "for j in author:\n",
    "    Authors.append(j.text.strip())\n",
    "\n",
    "for k in published_date:\n",
    "    Published_Date.append(k.text.split()) \n",
    "    \n",
    "for l in soup.find_all('a'):\n",
    "    Paper_URL.append(l.get('href'))\n",
    "\n",
    "Paper_URL1 = Paper_URL[:25]   \n",
    "\n",
    "data = {'Name': Paper_Title, 'Authors': Authors, 'Published Date':Published_Date,'Paper URL':Paper_URL1}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1128eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e505205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
