{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a3d729",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da048c10",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. \n",
    "The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search \n",
    "for guitars.\n",
    "\n",
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your \n",
    "search results and save it in a data frame and csv. In case if any product has less than 3 pages in search \n",
    "results then scrape all the products available under that product name. Details to be scraped are: \"Brand \n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and \n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d16e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def search_and_scrape_amazon_products(product):\n",
    "    driver = webdriver.Chrome(r'C:\\Users\\amuly\\Desktop\\New folder\\projects\\chromedriver.exe')\n",
    "    driver.get(\"https://www.amazon.in/\")\n",
    "    search_box = driver.find_element(\"id\", \"twotabsearchtextbox\")\n",
    "    search_box.send_keys(product)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the search results to load\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    products_data = []\n",
    "    page_count = 0\n",
    "\n",
    "    while page_count < 3:\n",
    "        # Find all the product elements\n",
    "        product_elements = driver.find_elements(\"css selector\", \"div.s-result-item\")\n",
    "\n",
    "        for element in product_elements:\n",
    "            product_data = {}\n",
    "            try:\n",
    "                product_data[\"Brand Name\"] = element.find_element(\"css selector\", \"span.a-size-base-plus.a-color-base.a-text-normal\").text\n",
    "            except:\n",
    "                product_data[\"Brand Name\"] = \"-\"\n",
    "            try:\n",
    "                product_data[\"Name of the Product\"] = element.find_element(\"css selector\", \"h2.a-size-mini.a-spacing-none.a-color-base.s-line-clamp-2\").text\n",
    "            except:\n",
    "                product_data[\"Name of the Product\"] = \"-\"\n",
    "            try:\n",
    "                product_data[\"Price\"] = element.find_element(\"css selector\", \"span.a-price-whole\").text\n",
    "            except:\n",
    "                product_data[\"Price\"] = \"-\"\n",
    "            try:\n",
    "                product_data[\"Return/Exchange\"] = element.find_element(\"css selector\", \"div.a-row.a-size-small span.a-size-small\").text\n",
    "            except:\n",
    "                product_data[\"Return/Exchange\"] = \"-\"\n",
    "            try:\n",
    "                product_data[\"Expected Delivery\"] = element.find_element(\"css selector\", \"span.a-text-bold span\").text\n",
    "            except:\n",
    "                product_data[\"Expected Delivery\"] = \"-\"\n",
    "            try:\n",
    "                product_data[\"Availability\"] = element.find_element(\"css selector\", \"span.a-size-medium span\").text\n",
    "            except:\n",
    "                product_data[\"Availability\"] = \"-\"\n",
    "            try:\n",
    "                product_data[\"Product URL\"] = element.find_element(\"css selector\", \"a.a-link-normal.a-text-normal\").get_attribute(\"href\")\n",
    "            except:\n",
    "                product_data[\"Product URL\"] = \"-\"\n",
    "\n",
    "            products_data.append(product_data)\n",
    "\n",
    "        # Go to the next page if available\n",
    "        try:\n",
    "            next_button = driver.find_element(\"css selector\", \"a[aria-label='Next']\")\n",
    "            next_button.click()\n",
    "            page_count += 1\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame(products_data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('amazon_products.csv', index=False)\n",
    "\n",
    "# Get the product from the user\n",
    "product = input(\"Enter the product to search for: \")\n",
    "search_and_scrape_amazon_products(product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26305e7d",
   "metadata": {},
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome(r'C:\\Users\\amuly\\Desktop\\New folder\\projects\\chromedriver.exe')\n",
    "for i in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "images = driver.find_elements(By.XPATH, '//IMG[@CLASS=\"RG_I Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "\n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i+1, 10))\n",
    "    response = requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\Users\\amuly\\Desktop\\New folder\\projects\" + str(i) + \".jpg\", \"wb\")\n",
    "    file.write(response.content)\n",
    "    file.close()\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c09d53",
   "metadata": {},
   "source": [
    "4.  Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on\n",
    "www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be \n",
    "scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, \n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the \n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48930396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "def search_and_scrape_flipkart_smartphones(product):\n",
    "    driver = webdriver.Chrome(r'C:\\Users\\amuly\\Desktop\\New folder\\projects\\chromedriver.exe')\n",
    "    driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "    # Close the login popup if it appears\n",
    "    try:\n",
    "        close_button = driver.find_element(\"css selector\", \"button._2KpZ6l._2doB4z\")\n",
    "        close_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Find the search box element and update the CSS selector accordingly\n",
    "    try:\n",
    "        search_box = driver.find_element(\"css selector\", \"#container > div > div._1kfTjk > div._1rH5Jn > div._2Xfa2_ > div._1cmsER > form > div > div > input\")\n",
    "        search_box.send_keys(product)\n",
    "        search_box.submit()\n",
    "    except:\n",
    "        print(\"Search box element not found.\")\n",
    "\n",
    "    # Wait for the search results to load\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Find all the product elements\n",
    "    product_elements = driver.find_elements(\"css selector\", \"div._1AtVbE\")\n",
    "\n",
    "    products_data = []\n",
    "\n",
    "    for element in product_elements:\n",
    "        product_data = {}\n",
    "\n",
    "        # Scrape the brand name\n",
    "        try:\n",
    "            brand = element.find_element(\"css selector\", \"div._4rR01T\").text\n",
    "        except:\n",
    "            brand = \"-\"\n",
    "\n",
    "        # Scrape the smartphone name\n",
    "        try:\n",
    "            name = element.find_element(\"css selector\", \"aIRf14\").text\n",
    "        except:\n",
    "            name = \"-\"\n",
    "\n",
    "        # Scrape the colour\n",
    "        try:\n",
    "            colour = element.find_element(\"css selector\", \"aIRf14\").text\n",
    "        except:\n",
    "            colour = \"-\"\n",
    "\n",
    "        # Scrape the RAM\n",
    "        try:\n",
    "            ram = element.find_element(\"css selector\", \"div._4rR01T\").text\n",
    "        except:\n",
    "            ram = \"-\"\n",
    "\n",
    "        # Scrape the storage (ROM)\n",
    "        try:\n",
    "            storage = element.find_element(\"css selector\", \"div._4rR01T\").text\n",
    "        except:\n",
    "            storage = \"-\"\n",
    "\n",
    "        # Scrape the primary camera\n",
    "        try:\n",
    "            primary_camera = element.find_element(\"css selector\", \"div._4rR01T\").text\n",
    "        except:\n",
    "            primary_camera = \"-\"\n",
    "\n",
    "        # Scrape the secondary camera\n",
    "        try:\n",
    "            secondary_camera = element.find_element(\"css selector\", \"div._4rR01T\").text\n",
    "        except:\n",
    "            secondary_camera = \"-\"\n",
    "\n",
    "        # Scrape the display size\n",
    "        try:\n",
    "            display_size = element.find_element(\"css selector\", \"div._4rR01T\").text\n",
    "        except:\n",
    "            display_size = \"-\"\n",
    "\n",
    "        # Scrape the battery capacity\n",
    "        try:\n",
    "            battery_capacity = element.find_element(\"css selector\", \"div._4rR01T\").text\n",
    "        except:\n",
    "            battery_capacity = \"-\"\n",
    "\n",
    "        # Scrape the price\n",
    "        try:\n",
    "            price = element.find_element(\"css selector\", \"div._30jeq3._1_WHN1\").text\n",
    "        except:\n",
    "            price = \"-\"\n",
    "\n",
    "        # Scrape the product URL\n",
    "        try:\n",
    "            product_url = element.find_element(\"css selector\", \"aIRf14\").get_attribute(\"href\")\n",
    "        except:\n",
    "            product_url = \"-\"\n",
    "\n",
    "        # Add the scraped data to the list\n",
    "        product_data[\"Brand Name\"] = brand\n",
    "        product_data[\"Smartphone Name\"] = name\n",
    "        product_data[\"Colour\"] = colour\n",
    "        product_data[\"RAM\"] = ram\n",
    "        product_data[\"Storage(ROM)\"] = storage\n",
    "        product_data[\"Primary Camera\"] = primary_camera\n",
    "        product_data[\"Secondary Camera\"] = secondary_camera\n",
    "        product_data[\"Display Size\"] = display_size\n",
    "        product_data[\"Battery Capacity\"] = battery_capacity\n",
    "        product_data[\"Price\"] = price\n",
    "        product_data[\"Product URL\"] = product_url\n",
    "\n",
    "        products_data.append(product_data)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    df = pd.DataFrame(products_data)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "    df.to_csv('flipkart_smartphones.csv', index=False)\n",
    "\n",
    "product = input(\"Enter the smartphone: \")\n",
    "search_and_scrape_flipkart_smartphones(product)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e15f0f",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google\n",
    "maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a32ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.google.com/maps\")\n",
    "\n",
    "input(\"https://www.google.com/maps/place/Bengaluru,+Karnataka/@12.9539456,77.4661266,11z/data=!3m1!4b1!4m6!3m5!1s0x3bae1670c9b44e6d:0xf8dfc3e8517e4fe0!8m2!3d12.9715987!4d77.5945627!16zL20vMDljMTc\")\n",
    "\n",
    "url_string = driver.current_url\n",
    "print(\"URL Extracted:\", url_string)\n",
    "\n",
    "lat_lng = re.findall(r'@(.*)data', url_string)\n",
    "\n",
    "if lat_lng:\n",
    "    coordinates = lat_lng[0].split(\",\")\n",
    "    latitude = coordinates[0]\n",
    "    longitude = coordinates[1]\n",
    "    print(\"Latitude:\", latitude)\n",
    "    print(\"Longitude:\", longitude)\n",
    "else:\n",
    "    print(\"Coordinates not found in the URL.\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e90b72b",
   "metadata": {},
   "source": [
    "6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) \n",
    "from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://trak.in/india-startup-funding-investment-2015/\"\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "table = soup.find(\"table\", {\"id\": \"tablepress-48\"})\n",
    "\n",
    "\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "\n",
    "for row in rows:\n",
    "   \n",
    "    cells = row.find_all(\"td\")\n",
    "    \n",
    "   \n",
    "    if len(cells) > 0:\n",
    "        # Extracting the deal details \n",
    "        date = cells[1].text.strip()\n",
    "        startup = cells[2].text.strip()\n",
    "        industry = cells[3].text.strip()\n",
    "        sub_vertical = cells[4].text.strip()\n",
    "        location = cells[5].text.strip()\n",
    "        investor = cells[6].text.strip()\n",
    "        investment_type = cells[7].text.strip()\n",
    "        amount = cells[8].text.strip()\n",
    "        \n",
    "      \n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"Startup: {startup}\")\n",
    "        print(f\"Industry: {industry}\")\n",
    "        print(f\"Sub-vertical: {sub_vertical}\")\n",
    "        print(f\"Location: {location}\")\n",
    "        print(f\"Investor: {investor}\")\n",
    "        print(f\"Investment Type: {investment_type}\")\n",
    "        print(f\"Amount: {amount}\")\n",
    "        print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d3c5e",
   "metadata": {},
   "source": [
    "7. Write a program to scrap all the available details of best gaming laptops from digit.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "laptop_divs = soup.find_all(\"div\", class_=\"TopNumbeHeading sticky-footer\")\n",
    "\n",
    "\n",
    "for laptop_div in laptop_divs:\n",
    "    \n",
    "    laptop_name = laptop_div.text.strip()\n",
    "    \n",
    "   \n",
    "    laptop_details_div = laptop_div.parent\n",
    "    \n",
    "    \n",
    "    specifications = laptop_details_div.find_all(\"div\", class_=\"Spcs-details\")\n",
    "    \n",
    "    \n",
    "    laptop_details = {}\n",
    "    laptop_details[\"Name\"] = laptop_name\n",
    "    \n",
    "    for spec in specifications:\n",
    "        spec_title = spec.find(\"div\", class_=\"heading\").text.strip()\n",
    "        spec_value = spec.find(\"div\", class_=\"value\").text.strip()\n",
    "        laptop_details[spec_title] = spec_value\n",
    "    \n",
    "   \n",
    "    for key, value in laptop_details.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ad151",
   "metadata": {},
   "source": [
    "8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be \n",
    "scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eaf3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.forbes.com/billionaires/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "billionaires_div = soup.find(\"div\", class_=\"personList\")\n",
    "rows = billionaires_div.find_all(\"div\", class_=\"person\")\n",
    "\n",
    "for row in rows:\n",
    "    rank = row.find(\"div\", class_=\"rank\").text.strip()\n",
    "    name = row.find(\"div\", class_=\"name\").text.strip()\n",
    "    net_worth = row.find(\"div\", class_=\"netWorth\").text.strip()\n",
    "    age = row.find(\"div\", class_=\"age\").text.strip()\n",
    "    citizenship = row.find(\"div\", class_=\"countryOfCitizenship\").text.strip()\n",
    "    source = row.find(\"div\", class_=\"source\").text.strip()\n",
    "    industry = row.find(\"div\", class_=\"category\").text.strip()\n",
    "    \n",
    "    print(f\"Rank: {rank}\")\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Net Worth: {net_worth}\")\n",
    "    print(f\"Age: {age}\")\n",
    "    print(f\"Citizenship: {citizenship}\")\n",
    "    print(f\"Source: {source}\")\n",
    "    print(f\"Industry: {industry}\")\n",
    "    print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4cbf30",
   "metadata": {},
   "source": [
    "9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted \n",
    "from any YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8422afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "service = Service(r'C:\\Users\\amuly\\Desktop\\New folder\\projects\\chromedriver.exe')\n",
    "\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "video_url = 'https://www.youtube.com/watch?v=YOUR_VIDEO_ID'\n",
    "driver.get(video_url)\n",
    "\n",
    "scroll_pause_time = 2\n",
    "scroll_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    time.sleep(scroll_pause_time)\n",
    "    new_scroll_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "    if new_scroll_height == scroll_height:\n",
    "        break\n",
    "\n",
    "    scroll_height = new_scroll_height\n",
    "\n",
    "comments = driver.find_elements(By.CSS_SELECTOR, '#content-text')\n",
    "upvotes = driver.find_elements(By.CSS_SELECTOR, '#vote-count-middle')\n",
    "times = driver.find_elements(By.CSS_SELECTOR, '#header-author > yt-formatted-string > a > span')\n",
    "\n",
    "while len(comments) < 500:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    time.sleep(scroll_pause_time)\n",
    "    comments = driver.find_elements(By.CSS_SELECTOR, '#content-text')\n",
    "    upvotes = driver.find_elements(By.CSS_SELECTOR, '#vote-count-middle')\n",
    "    times = driver.find_elements(By.CSS_SELECTOR, '#header-author > yt-formatted-string > a > span')\n",
    "\n",
    "for i in range(len(comments)):\n",
    "    comment = comments[i].text.strip()\n",
    "    upvote = upvotes[i].text.strip()\n",
    "    time_posted = times[i].text.strip()\n",
    "    print(f\"Comment: {comment}\")\n",
    "    print(f\"Upvotes: {upvote}\")\n",
    "    print(f\"Time Posted: {time_posted}\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f054c77",
   "metadata": {},
   "source": [
    "10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in \n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, \n",
    "overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9567044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = \"https://www.hostelworld.com/search?search_keywords=London,%20England&country=England&city=London&type=city&id=3\"\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "hostel_containers = soup.find_all(\"div\", class_=\"fabresult\")\n",
    "\n",
    "for container in hostel_containers:\n",
    "    hostel_name = container.find(\"h2\", class_=\"title\").text.strip()\n",
    "    distance = container.find(\"span\", class_=\"distance\").text.strip()\n",
    "    rating = container.find(\"div\", class_=\"score orange\").text.strip()\n",
    "    total_reviews = container.find(\"div\", class_=\"reviews\").text.strip().split()[0]\n",
    "    overall_reviews = container.find(\"div\", class_=\"rating\").text.strip()\n",
    "    privates_from_price = container.find(\"div\", class_=\"price-col\").find(\"a\", class_=\"prices\").text.strip()\n",
    "    dorms_from_price = container.find(\"div\", class_=\"price-col\").find(\"a\", class_=\"prices\").find_next_sibling().text.strip()\n",
    "    facilities = container.find(\"ul\", class_=\"facilities\").text.strip().replace(\"\\n\", \", \")\n",
    "    property_description = container.find(\"div\", class_=\"ratings\").find_next_sibling(\"div\").text.strip()\n",
    "    \n",
    "\n",
    "    print(\"Hostel Name:\", hostel_name)\n",
    "    print(\"Distance from City Centre:\", distance)\n",
    "    print(\"Rating:\", rating)\n",
    "    print(\"Total Reviews:\", total_reviews)\n",
    "    print(\"Overall Reviews:\", overall_reviews)\n",
    "    print(\"Privates From Price:\", privates_from_price)\n",
    "    print(\"Dorms From Price:\", dorms_from_price)\n",
    "    print(\"Facilities:\", facilities)\n",
    "    print(\"Property Description:\", property_description)\n",
    "    print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405d63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
