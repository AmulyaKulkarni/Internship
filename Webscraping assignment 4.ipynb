{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b374ca43",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f248c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rank                                             Name  \\\n",
      "0    1.                            \"Baby Shark Dance\"[4]   \n",
      "1    2.                                   \"Despacito\"[7]   \n",
      "2    3.                       \"Johny Johny Yes Papa\"[14]   \n",
      "3    4.                                  \"Bath Song\"[15]   \n",
      "4    5.                               \"Shape of You\"[16]   \n",
      "5    6.                              \"See You Again\"[19]   \n",
      "6    7.                \"Phonics Song with Two Words\"[24]   \n",
      "7    8.                          \"Wheels on the Bus\"[25]   \n",
      "8    9.                                \"Uptown Funk\"[26]   \n",
      "9   10.  \"Learning Colors – Colorful Eggs on a Farm\"[27]   \n",
      "10  11.                              \"Gangnam Style\"[28]   \n",
      "11  12.   \"Masha and the Bear – Recipe for Disaster\"[33]   \n",
      "12  13.                             \"Dame Tu Cosita\"[34]   \n",
      "13  14.                                     \"Axel F\"[35]   \n",
      "14  15.                                      \"Sugar\"[36]   \n",
      "15  16.                                       \"Roar\"[37]   \n",
      "16  17.                             \"Counting Stars\"[38]   \n",
      "17  18.                                      \"Sorry\"[39]   \n",
      "18  19.                        \"Baa Baa Black Sheep\"[40]   \n",
      "19  20.                          \"Thinking Out Loud\"[41]   \n",
      "20  21.           \"Waka Waka (This Time for Africa)\"[42]   \n",
      "21  22.                                 \"Dark Horse\"[43]   \n",
      "22  23.                             \"Lakdi Ki Kathi\"[44]   \n",
      "23  24.                                      \"Faded\"[45]   \n",
      "24  25.                                    \"Perfect\"[46]   \n",
      "25  26.                                 \"Let Her Go\"[47]   \n",
      "26  27.                             \"Girls Like You\"[48]   \n",
      "27  28.          \"Humpty the train on a fruits ride\"[49]   \n",
      "28  29.                                    \"Lean On\"[50]   \n",
      "29  30.                                   \"Bailando\"[51]   \n",
      "\n",
      "                                           Artist        Upload Date  Views  \n",
      "0     Pinkfong Baby Shark - Kids' Songs & Stories      June 17, 2016  12.85  \n",
      "1                                      Luis Fonsi   January 12, 2017   8.16  \n",
      "2                                     LooLoo Kids    October 8, 2016   6.70  \n",
      "3                      Cocomelon – Nursery Rhymes        May 2, 2018   6.20  \n",
      "4                                      Ed Sheeran   January 30, 2017   6.00  \n",
      "5                                     Wiz Khalifa      April 6, 2015   5.89  \n",
      "6                                       ChuChu TV      March 6, 2014   5.30  \n",
      "7                      Cocomelon – Nursery Rhymes       May 24, 2018   5.24  \n",
      "8                                     Mark Ronson  November 19, 2014   4.92  \n",
      "9                                     Miroshka TV  February 27, 2018   4.89  \n",
      "10                                            Psy      July 15, 2012   4.80  \n",
      "11                                     Get Movies   January 31, 2012   4.55  \n",
      "12                                      El Chombo      April 5, 2018   4.35  \n",
      "13                                     Crazy Frog      June 16, 2009   3.91  \n",
      "14                                       Maroon 5   January 14, 2015   3.87  \n",
      "15                                     Katy Perry  September 5, 2013   3.80  \n",
      "16                                    OneRepublic       May 31, 2013   3.79  \n",
      "17                                  Justin Bieber   October 22, 2015   3.66  \n",
      "18                     Cocomelon – Nursery Rhymes      June 25, 2018   3.64  \n",
      "19                                     Ed Sheeran    October 7, 2014   3.60  \n",
      "20                                        Shakira       June 4, 2010   3.59  \n",
      "21                                     Katy Perry  February 20, 2014   3.52  \n",
      "22                                   Jingle Toons      June 14, 2018   3.48  \n",
      "23                                    Alan Walker   December 3, 2015   3.45  \n",
      "24                                     Ed Sheeran   November 9, 2017   3.45  \n",
      "25                                      Passenger      July 25, 2012   3.44  \n",
      "26                                       Maroon 5       May 31, 2018   3.42  \n",
      "27  Kiddiestv Hindi – Nursery Rhymes & Kids Songs   January 26, 2018   3.41  \n",
      "28                                    Major Lazer     March 22, 2015   3.38  \n",
      "29                               Enrique Iglesias     April 11, 2014   3.38  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "\n",
    "chromedriver_path = r'C:\\Users\\amuly\\Desktop\\New folder\\projects\\chromedriver.exe'\n",
    "\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "\n",
    "    table = driver.find_element(By.CLASS_NAME, \"wikitable.sortable\")\n",
    "\n",
    "    rows = table.find_elements(By.TAG_NAME, \"tr\")[1:]\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        columns = row.find_elements(By.TAG_NAME, \"td\")\n",
    "\n",
    "        if len(columns) >= 5:\n",
    "            rank = columns[0].text.strip()\n",
    "            name = columns[1].text.strip()\n",
    "            artist = columns[2].text.strip()\n",
    "            upload_date = columns[4].text.strip()\n",
    "            views = columns[3].text.strip()\n",
    "\n",
    "            data.append([rank, name, artist, upload_date, views])\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Rank', 'Name', 'Artist', 'Upload Date', 'Views'])\n",
    "\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "except WebDriverException as e:\n",
    "    print(\"An error occurred\")\n",
    "\n",
    "finally:\n",
    "   \n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba9243",
   "metadata": {},
   "source": [
    "2. Scrape the details teamIndia’sinternationalfixtures from bcci.tv. \n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1stODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98d49d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "International dropdown not found\n",
      "Fixtures option not found\n",
      "Error\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "\n",
    "options = Options()\n",
    "options.headless = True  \n",
    "\n",
    "\n",
    "chromedriver_path = r'C:\\Users\\amuly\\Desktop\\New folder\\projects\\chromedriver.exe'\n",
    "\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "try:\n",
    "   \n",
    "    driver.get(\"https://www.bcci.tv/\")\n",
    "\n",
    "    try:\n",
    "       \n",
    "        international_dropdown = WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, \"//a[@href='/international']\"))\n",
    "        )\n",
    "\n",
    "       \n",
    "        international_dropdown.click()\n",
    "    except TimeoutException:\n",
    "        print(\"International dropdown not found\")\n",
    "       \n",
    "\n",
    "    try:\n",
    "       \n",
    "        fixtures_option = WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, \"//a[@href='/international/fixtures']\"))\n",
    "        )\n",
    "\n",
    "       \n",
    "        fixtures_option.click()\n",
    "    except TimeoutException:\n",
    "        print(\"Fixtures option not found\")\n",
    "       \n",
    "\n",
    "\n",
    "    fixture_container = driver.find_element(By.CLASS_NAME, \"js-list\")\n",
    "\n",
    "    fixtures = fixture_container.find_elements(By.CLASS_NAME, \"list-item\")\n",
    "\n",
    "  \n",
    "    for fixture in fixtures:\n",
    "        try:\n",
    "           \n",
    "            match_title = fixture.find_element(By.CLASS_NAME, \"fixture-date\").text.strip()\n",
    "            series = fixture.find_element(By.CLASS_NAME, \"fixture-name\").text.strip()\n",
    "            place = fixture.find_element(By.CLASS_NAME, \"fixture-location\").text.strip()\n",
    "            date = fixture.find_element(By.CLASS_NAME, \"fixture-date\").get_attribute(\"data-date\")\n",
    "            time = fixture.find_element(By.CLASS_NAME, \"fixture-date\").get_attribute(\"data-time\")\n",
    "\n",
    "         \n",
    "            print(\"Match Title:\", match_title)\n",
    "            print(\"Series:\", series)\n",
    "            print(\"Place:\", place)\n",
    "            print(\"Date:\", date)\n",
    "            print(\"Time:\", time)\n",
    "            print(\"----------------------------------\")\n",
    "\n",
    "        except WebDriverException as e:\n",
    "            print(\"WebDriverException occurred while processing \")\n",
    "\n",
    "except WebDriverException as e:\n",
    "    print(\"Error\")\n",
    "\n",
    "finally:\n",
    " \n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de153bf",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP ofIndia fromstatisticstime.com. \n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7b952a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element not found\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException  \n",
    "\n",
    "\n",
    "options = Options()\n",
    "options.headless = True  \n",
    "\n",
    "chromedriver_path = r'C:\\Users\\amuly\\Desktop\\New folder\\projects\\chromedriver.exe'\n",
    "\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "try:\n",
    "    \n",
    "    driver.get(\"http://statisticstimes.com/\")\n",
    "\n",
    "    economy_link = driver.find_element(By.XPATH, \"//div[@class='navbar']//a[contains(text(), 'Economy')]\")\n",
    "    economy_link.click()\n",
    "\n",
    "    gdp_link = driver.find_element(By.XPATH, \"//div[@class='container1']//ul[@style='list-style-type:none;margin-left:20px;']//li/a[contains(text(), 'GDP of Indian states')]\")\n",
    "    gdp_link.click()\n",
    "\n",
    "  \n",
    "    gdp_table = driver.find_element(By.XPATH, \"//table[@id='table_id']\")\n",
    "\n",
    "   \n",
    "    rows = gdp_table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "\n",
    "    for row in rows[1:]:\n",
    "       \n",
    "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "\n",
    "       \n",
    "        rank = cells[0].text.strip()\n",
    "        state = cells[1].text.strip()\n",
    "        gdp_1819 = cells[2].text.strip()\n",
    "        gdp_1920 = cells[3].text.strip()\n",
    "        share_1819 = cells[4].text.strip()\n",
    "        gdp_billion = cells[5].text.strip()\n",
    "\n",
    "      \n",
    "        print(\"Rank:\", rank)\n",
    "        print(\"State:\", state)\n",
    "        print(\"GSDP(18-19) - at current prices:\", gdp_1819)\n",
    "        print(\"GSDP(19-20) - at current prices:\", gdp_1920)\n",
    "        print(\"Share(18-19):\", share_1819)\n",
    "        print(\"GDP($ billion):\", gdp_billion)\n",
    "        print(\"----------------------------------\")\n",
    "\n",
    "except NoSuchElementException as e:\n",
    "    print(\"Element not found\")\n",
    "\n",
    "except Exception as e:\n",
    "    \n",
    "    print(\"Error occurred\")\n",
    "\n",
    "finally:\n",
    "\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21c44b",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91e773f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The browser window was closed or the WebDriver lost connection.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchWindowException\n",
    "\n",
    "driver_path = r'C:\\Users\\amuly\\Desktop\\New folder\\projects\\chromedriver.exe'\n",
    "\n",
    "service = Service(driver_path)\n",
    "\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "try:\n",
    "    driver.get('https://github.com/')\n",
    "\n",
    "    explore_menu = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.LINK_TEXT, 'Explore'))\n",
    "    )\n",
    "    explore_menu.click()\n",
    "\n",
    "    trending_option = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.LINK_TEXT, 'Trending'))\n",
    "    )\n",
    "    trending_option.click()\n",
    "\n",
    "    repositories = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'article.Box-row'))\n",
    "    )\n",
    "\n",
    "    for repo in repositories:\n",
    "        title_element = repo.find_element(By.CSS_SELECTOR, 'h1.h3')\n",
    "        repository_title = title_element.text.strip()\n",
    "\n",
    "        description_element = repo.find_element(By.CSS_SELECTOR, 'p.my-1')\n",
    "        repository_description = description_element.text.strip()\n",
    "\n",
    "        contributors_element = repo.find_element(By.CSS_SELECTOR, 'a.muted-link')\n",
    "        contributors_count = contributors_element.text.strip()\n",
    "\n",
    "        language_element = repo.find_element(By.CSS_SELECTOR, 'span[itemprop=\"programmingLanguage\"]')\n",
    "        language_used = language_element.text.strip() if language_element else \"Not specified\"\n",
    "\n",
    "        print(f\"Repository Title: {repository_title}\")\n",
    "        print(f\"Repository Description: {repository_description}\")\n",
    "        print(f\"Contributors Count: {contributors_count}\")\n",
    "        print(f\"Language Used: {language_used}\")\n",
    "\n",
    "except NoSuchWindowException:\n",
    "    print(\"The browser window was closed or the WebDriver lost connection.\")\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7e216",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com. \n",
    "Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc409f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The browser window was closed or the WebDriver lost connection.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchWindowException\n",
    "\n",
    "driver_path = r'C:\\Users\\amuly\\Desktop\\New folder\\projects\\chromedriver.exe'\n",
    "\n",
    "# Create a Service object using the ChromeDriver executable path\n",
    "service = Service(driver_path)\n",
    "\n",
    "# Create a WebDriver instance using the Service\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "try:\n",
    "    driver.get('https://www.billboard.com/')\n",
    "\n",
    "    charts_link = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.LINK_TEXT, 'Charts'))\n",
    "    )\n",
    "    charts_link.click()\n",
    "\n",
    "    hot_100_link = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.LINK_TEXT, 'Hot 100'))\n",
    "    )\n",
    "    hot_100_link.click()\n",
    "\n",
    "    table = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'chart-list'))\n",
    "    )\n",
    "\n",
    "    rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "    for row in rows:\n",
    "        cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "        if len(cells) == 5:\n",
    "            song_name = cells[1].text\n",
    "            artist_name = cells[2].text\n",
    "            last_week_rank = cells[3].text\n",
    "            peak_rank = cells[4].text\n",
    "            weeks_on_board = cells[5].text\n",
    "\n",
    "            print(f\"Song: {song_name}\")\n",
    "            print(f\"Artist: {artist_name}\")\n",
    "            print(f\"Last Week Rank: {last_week_rank}\")\n",
    "            print(f\"Peak Rank: {peak_rank}\")\n",
    "            print(f\"Weeks on Board: {weeks_on_board}\")\n",
    "\n",
    "except NoSuchWindowException:\n",
    "    print(\"The browser window was closed or the WebDriver lost connection.\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4f863",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest sellingnovels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4fe92dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n",
      "   Book Name                                        Author Name  \\\n",
      "0          1                                  Da Vinci Code,The   \n",
      "1          2               Harry Potter and the Deathly Hallows   \n",
      "2          3           Harry Potter and the Philosopher's Stone   \n",
      "3          4          Harry Potter and the Order of the Phoenix   \n",
      "4          5                               Fifty Shades of Grey   \n",
      "..       ...                                                ...   \n",
      "95        96                                          Ghost,The   \n",
      "96        97                     Happy Days with the Naked Chef   \n",
      "97        98              Hunger Games,The:Hunger Games Trilogy   \n",
      "98        99  Lost Boy,The:A Foster Child's Search for the L...   \n",
      "99       100  Jamie's Ministry of Food:Anyone Can Learn to C...   \n",
      "\n",
      "        Volumes Sold  Publisher            Genre  \n",
      "0         Brown, Dan  5,094,805       Transworld  \n",
      "1      Rowling, J.K.  4,475,152       Bloomsbury  \n",
      "2      Rowling, J.K.  4,200,654       Bloomsbury  \n",
      "3      Rowling, J.K.  4,179,479       Bloomsbury  \n",
      "4       James, E. L.  3,758,936     Random House  \n",
      "..               ...        ...              ...  \n",
      "95    Harris, Robert    807,311     Random House  \n",
      "96     Oliver, Jamie    794,201          Penguin  \n",
      "97  Collins, Suzanne    792,187  Scholastic Ltd.  \n",
      "98      Pelzer, Dave    791,507            Orion  \n",
      "99     Oliver, Jamie    791,095          Penguin  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "table = soup.find('table', class_='in-article sortable')\n",
    "\n",
    "rows = table.find_all('tr')[1:]\n",
    "\n",
    "data = []\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "\n",
    "    if len(cells) >= 5:\n",
    "        book_name = cells[0].text.strip()\n",
    "        author_name = cells[1].text.strip()\n",
    "        volumes_sold = cells[2].text.strip()\n",
    "        publisher = cells[3].text.strip()\n",
    "        genre = cells[4].text.strip()\n",
    "\n",
    "        data.append([book_name, author_name, volumes_sold, publisher, genre])\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Book Name', 'Author Name', 'Volumes Sold', 'Publisher', 'Genre'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619f758",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com. \n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c3745a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Name    Year Span                     Genre  \\\n",
      "0                  Game of Thrones  (2011–2019)  Action, Adventure, Drama   \n",
      "1                  Stranger Things  (2016–2024)    Drama, Fantasy, Horror   \n",
      "2                 The Walking Dead  (2010–2022)   Drama, Horror, Thriller   \n",
      "3                   13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   \n",
      "4                          The 100  (2014–2020)    Drama, Mystery, Sci-Fi   \n",
      "..                             ...          ...                       ...   \n",
      "95                           Reign  (2013–2017)                     Drama   \n",
      "96  A Series of Unfortunate Events  (2017–2019)  Adventure, Comedy, Drama   \n",
      "97                  Criminal Minds     (2005– )     Crime, Drama, Mystery   \n",
      "98                          Scream  (2015–2019)      Comedy, Crime, Drama   \n",
      "99      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
      "\n",
      "   Run Time Ratings      Votes  \n",
      "0    57 min     9.2  2,163,377  \n",
      "1    51 min     8.7  1,243,199  \n",
      "2    44 min     8.1  1,027,704  \n",
      "3    60 min     7.5    302,304  \n",
      "4    43 min     7.6    261,418  \n",
      "..      ...     ...        ...  \n",
      "95   42 min     7.4     51,692  \n",
      "96   50 min     7.8     63,735  \n",
      "97   42 min     8.1    207,730  \n",
      "98   45 min       7     43,226  \n",
      "99  572 min     8.6    258,372  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.imdb.com/list/ls095964455/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "series_items = soup.find_all('div', class_='lister-item mode-detail')\n",
    "\n",
    "data = []\n",
    "for item in series_items:\n",
    "    try:\n",
    "        name = item.h3.a.text.strip()\n",
    "        year_span = item.find('span', class_='lister-item-year').text.strip()\n",
    "        genre = item.find('span', class_='genre').text.strip()\n",
    "        run_time = item.find('span', class_='runtime').text.strip()\n",
    "        ratings = item.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "        votes = item.find('span', attrs={'name': 'nv'}).text.strip()\n",
    "\n",
    "        data.append([name, year_span, genre, run_time, ratings, votes])\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Name', 'Year Span', 'Genre', 'Run Time', 'Ratings', 'Votes'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af54285",
   "metadata": {},
   "source": [
    "8. Details of Datasetsfrom UCI machine learning repositories. \n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65c3a9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShowAllDataset link not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "try:\n",
    "    show_all_link = soup.find('a', href='showall.php')\n",
    "\n",
    "    if show_all_link is not None:\n",
    "        show_all_url = url + show_all_link['href']\n",
    "\n",
    "        show_all_response = requests.get(show_all_url)\n",
    "        show_all_soup = BeautifulSoup(show_all_response.text, 'html.parser')\n",
    "\n",
    "        rows = show_all_soup.find_all('tr')[1:]\n",
    "\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "\n",
    "            dataset_name = cells[0].text.strip()\n",
    "            data_type = cells[1].text.strip()\n",
    "            task = cells[2].text.strip()\n",
    "            attribute_type = cells[3].text.strip()\n",
    "            num_instances = cells[4].text.strip()\n",
    "            num_attributes = cells[5].text.strip()\n",
    "            year = cells[6].text.strip()\n",
    "\n",
    "            print(f\"Dataset Name: {dataset_name}\")\n",
    "            print(f\"Data Type: {data_type}\")\n",
    "            print(f\"Task: {task}\")\n",
    "            print(f\"Attribute Type: {attribute_type}\")\n",
    "            print(f\"No. of Instances: {num_instances}\")\n",
    "            print(f\"No. of Attributes: {num_attributes}\")\n",
    "            print(f\"Year: {year}\")\n",
    "            print(\"------------------------------\")\n",
    "    else:\n",
    "        print(\"ShowAllDataset link not found.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098513f",
   "metadata": {},
   "source": [
    "9. Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants\n",
    "You have to find the following details: \n",
    "A) Name\n",
    "B) Designation\n",
    "C)Company \n",
    "D)Skills they hire for \n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and \n",
    "click on search. All this should be done through cod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca2e1238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recruiters link not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.naukri.com/hr-recruiters-consultants'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "try:\n",
    "    recruiters_link = soup.find('a', text='Recruiters')\n",
    "\n",
    "    if recruiters_link is not None:\n",
    "        recruiters_url = recruiters_link['href']\n",
    "\n",
    "        recruiters_response = requests.get(recruiters_url)\n",
    "        recruiters_soup = BeautifulSoup(recruiters_response.text, 'html.parser')\n",
    "\n",
    "        search_input = recruiters_soup.find('input', id='rootSearch')\n",
    "\n",
    "        if search_input is not None:\n",
    "            search_input['value'] = 'Data Science'\n",
    "            recruiters_soup.find('form', id='rootForm').submit()\n",
    "\n",
    "            rows = recruiters_soup.find_all('div', class_='recInfo')\n",
    "\n",
    "            data = []\n",
    "            for row in rows:\n",
    "                name = row.find('span', class_='fl').text.strip()\n",
    "                designation = row.find('span', class_='designation').text.strip()\n",
    "                company = row.find('p', class_='highlightable').text.strip()\n",
    "                skills = row.find('div', class_='recSkills').text.strip()\n",
    "                location = row.find('small').text.strip()\n",
    "\n",
    "                data.append([name, designation, company, skills, location])\n",
    "\n",
    "            df = pd.DataFrame(data, columns=['Name', 'Designation', 'Company', 'Skills', 'Location'])\n",
    "            print(df)\n",
    "        else:\n",
    "            print(\"Search input field not found.\")\n",
    "    else:\n",
    "        print(\"Recruiters link not found.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d9eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
